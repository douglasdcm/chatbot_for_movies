{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Este notebook apresenta a exploração dos dados após do pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters for data visualization\n",
    "np.set_printoptions(threshold=None, precision=2)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('./chatdata/movie_lines_pre_processed_keras.tsv', header = None, delimiter=\"\\t\", quoting=3, encoding='ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.columns = ['msg_line', 'user_id', 'movie_id', 'msg', 'msg_2', 'msg_pre_processed', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(messages, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis based on this article\n",
    "- https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools?utm_source=medium&utm_medium=crosspost&utm_campaign=blog-exploratory-data-analysis-natural-language-processing-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = messages['msg_pre_processed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters of each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram to display the number of character of each message\n",
    "data.str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of characters are between 0 and 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the average word length\n",
    "data_set = [type(item) for item in data]\n",
    "data_set = set(data_set)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_val = [it for it in data if isinstance(it, float)]\n",
    "print(len(float_val))\n",
    "float_val = set(float_val)\n",
    "print(float_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[messages['msg_pre_processed'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-alphabetical messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of sequence of null messages\n",
    "#messages[538:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the nan messages with a string\n",
    "messages = messages.fillna('UNKNOWN')\n",
    "data = messages['msg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words for each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words for each message\n",
    "data.str.split(' ').\\\n",
    "    map(lambda x: len(str(x))).\\\n",
    "    hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words are between 0 and 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the average word length\n",
    "data.str.split(' ').\\\n",
    "   apply(lambda x : [len(i) for i in x]). \\\n",
    "   map(lambda x: np.mean(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of words goes from 0 to 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuition of stopwords in the mesages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkig the distribuition of stopwrds in the mesages\n",
    "stop=set(stopwords.words('english'))\n",
    "corpus=[]\n",
    "msg_ = data.str.split()\n",
    "msg_ = msg_.values.tolist()\n",
    "corpus=[word for i in msg_ for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dic.keys()\n",
    "y = dic.values()\n",
    "plt.subplots(figsize=(30,5))\n",
    "plt.bar(x, y, 1, color='b')\n",
    "plt.xticks(list(x), rotation=90, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of stop words are 'to', 'you', 'the' and 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting the occurences of each word\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word 'I' has the biggest occurrence. There are a lot of messages like dashes that can be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most frequent n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the most frequent n-grams\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_gram(corpus, n=2):\n",
    "    top_n_bigrams=get_top_ngram(corpus,n)[:10]\n",
    "    x,y=map(list,zip(*top_n_bigrams))\n",
    "    sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_gram(data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more frequent bigram is 'you re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_gram(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of short questions like 'what do you...?', 'what are you...?', and answers like 'you want to...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_gram(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of repeated questions and answers or part of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkig the polarity of the messages\n",
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['polarity_score']=data.\\\n",
    "   apply(lambda x : polarity(x))\n",
    "messages['polarity_score'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of polarity is neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TYPE\tDESCRIPTION\n",
    "- PERSON\tPeople, including fictional.\n",
    "- NORP\tNationalities or religious or political groups.\n",
    "- FAC\tBuildings, airports, highways, bridges, etc.\n",
    "- ORG\tCompanies, agencies, institutions, etc.\n",
    "- GPE\tCountries, cities, states.\n",
    "- LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART\tTitles of books, songs, etc.\n",
    "- LAW\tNamed documents made into laws.\n",
    "- LANGUAGE\tAny named language.\n",
    "- DATE\tAbsolute or relative dates or periods.\n",
    "- TIME\tTimes smaller than a day.\n",
    "- PERCENT\tPercentage, including ”%“.\n",
    "- MONEY\tMonetary values, including unit.\n",
    "- QUANTITY\tMeasurements, as of weight or distance.\n",
    "- ORDINAL\t“first”, “second”, etc.\n",
    "- CARDINAL\tNumerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the messages in a big document\n",
    "def get_entities(corpus, int_limit=0, end_limit=100):\n",
    "    msg_all = ' '.join(data[int_limit:end_limit])\n",
    "    doc = nlp(msg_all)\n",
    "\n",
    "    entity_list = [(x.text,x.label_) for x in doc.ents]\n",
    "    \n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ent_list = set()\n",
    "skip = 1000\n",
    "i = 0\n",
    "lim = len(data)\n",
    "#lim = 20000\n",
    "\n",
    "while(i < lim):    \n",
    "    ent_list = set(get_entities(data, i, i+skip))\n",
    "    ent_list.union(ent_list)\n",
    "    i = i + skip\n",
    "    print('Processed '+str(i)+' of '+str(lim)+' messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some personal names, real locals, name of organizations and work of art that can be removed or replaced for generic terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = pd.DataFrame(list(ent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "    doc=nlp(text)\n",
    "    return [X.label_ for X in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#count the number of each entity\n",
    "ent=data.\\\n",
    "    apply(lambda x : ner(x))\n",
    "ent=[x for sub in ent for x in sub]\n",
    "\n",
    "counter=Counter(ent)\n",
    "count=counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=map(list,zip(*count))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most comon tokens per entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text,ent=\"PERSON\"):\n",
    "    doc=nlp(text)\n",
    "    return [X.text for X in doc.ents if X.label_ == ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#count the tokens for a given entity type\n",
    "gpe=data.apply(lambda x: ner(x, 'PERSON'))\n",
    "gpe=[i for x in gpe for i in x]\n",
    "counter=Counter(gpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speach Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are eight main parts of speech:\n",
    "\n",
    "- Noun (NN)- Joseph, London, table, cat, teacher, pen, city\n",
    "- Verb (VB)- read, speak, run, eat, play, live, walk, have, like, are, is\n",
    "- Adjective(JJ)- beautiful, happy, sad, young, fun, three\n",
    "- Adverb(RB)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "- Preposition (IN)- at, on, in, from, with, near, between, about, under\n",
    "- Conjunction (CC)- and, or, but, because, so, yet, unless, since, if\n",
    "- Pronoun(PRP)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "- Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_spaces(corpus):\n",
    "    #remove duplicated spaces\n",
    "    corpus_alt = re.sub(r' +', ' ', corpus)\n",
    "\n",
    "    return corpus_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_no_space = [remove_duplicated_spaces(str(m)) for m in data]\n",
    "msg_no_space = pd.Series(msg_no_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(text):\n",
    "    #matched = re.match(r\" +\", text)\n",
    "    if text != ' ':\n",
    "        #print(text)\n",
    "        pos=nltk.pos_tag(word_tokenize(text))\n",
    "        #print(pos)\n",
    "        pos=list(map(list,zip(*pos)))[1]\n",
    "        return pos\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tags=msg_no_space[0:5000].apply(lambda x : pos(x))\n",
    "#tags = [pos(m) for m in msg_no_space[0:10000]]\n",
    "#tags\n",
    "tags=[x for l in tags for x in l]\n",
    "counter=Counter(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=list(map(list,zip(*counter.most_common(7))))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### most commonly nouns\n",
    "def get_adjs(text, _tag='PRP'):\n",
    "    adj=[]\n",
    "    pos=nltk.pos_tag(word_tokenize(text))\n",
    "    for word,tag in pos:\n",
    "        if tag == _tag:\n",
    "            adj.append(word)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words=data.apply(lambda x : get_adjs(x))\n",
    "words=[x for l in words for x in l]\n",
    "counter=Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=list(map(list,zip(*counter.most_common(7))))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text complexy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch Reading Ease (FRE)\n",
    "<br>Higher scores indicate material that is easier to read, lower numbers mark harder-to-read passages:\n",
    "- 0-30 College\n",
    "- 50-60 High school\n",
    "- 60+ Fourth grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reading = data.\\\n",
    "   apply(lambda x : flesch_reading_ease(x))\n",
    "reading.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[i for i in range(len(reading)) if reading[i]<0]\n",
    "\n",
    "messages.iloc[x]['msg_pre_processed'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
